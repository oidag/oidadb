#+SETUPFILE: ~/.emacs.d/themes/stylish_white.theme
#+TITLE: OidaDB Specification
#+AUTHOR: Kevin Marschke

Let us manage a list of all page versions in the database header. So
the header offset of 123 will be the version of page offset 123.

* Core library
The core part of oidadb is its pages interface. This interface is
inherited into more specific things such as the index, lookups,
fixeddat, dyndat, ect. But All of these things all use the pages
interface to perform their operations and expose interfaces simular to
that.

v1 will be JUST pages.


Inheritance tree (for how data is structured, i think):

Pages <- Entities <- Fixed Data    <- btrees
                                   <- structure data
                  <- Dynamic Data  <- ?also structured data?
      <- ?Blobs

** Thoughts on permissions

Further down the line, I can add permissions to the overall
interface. That is, every time checkout is called, it creates a
"namespace" so child checkouts will have to follow the rules of that
namespace. (think capabilities).


how about the idea of *rejecting* merges by the parent. this will
provide also the ability to accept them and call hooks.

* Descriptor Engineering
For an open descriptor of the database, we want to make sure we use
the idea of commits, merges, ect.

 - Open a file descriptor.
 - Performing any operations without checking out is editing data
   strait on the file. This may be useful to you, it may not be. But
   it is allowed.
 - Checkout - When you checkout, its like you freeze the database from
   being edited by anything else. You won't see other's changes during
   this time (unless their your own).
 - Use - when you specify that you want to "use" a page, this means
   that you are declaring the uses you need for a particular page. You
   must specify the use of a page before you can modify/see its
   contents. All calls to "use" add it to the checkout stack and are
   all closed when a commit is successfully executed. You can also
   specify a use to be a "full exclusive" - this will stop others from
   accessing that page but also guanetee that no conflict can happen
 - Commit - if the commit fails, a list of conflicts is shown by means
   of a list of pages that have conflicts. Commit will not work until
   these conflicts are marked as resolved. You can compare conflicts
   between your pages (via odbh_page) and the upstream pages (that
   conflict will return, but these upstream pages are readonly).
 - Rollback - forget about all changes.


You can call "checkout" multiple times to have recursive checkout frames.

Once again, you can operate outside the checkout. 

** Thoughts on networking

With the "use" and checkout stuff. we can change the checkout/commit
and stuff to operate through a socket instead of shared memory

* This Specification
** Reserved Fields
Various parts in this specification you'll see some fields labelled
for reserved use. These fields are labelled as ~rsvd0~, ~rsvd1~,
~rsvd2~, ect. These are here to denote fields that I'm not sure what
to do with at this time. In practice, you - as the implementor - can
technically use these fields however you want. If you have some crazy
idea that you'd like to implement using these reserved fields, then be
my guest. Just don't be surprised when a later version of this
specification uses them for something else.

Otherwise, always initialize reserved fields as 0's.

* Limitations
 - There can only be a maximum of 128 processes that perform
   /concurrent/ work on a given database. An unlimited amount of
   processes can have the database open, however, only 128 can perform
   commits at a time.
* ---- Static ----
* ODB File
** Overview and Layout
Inside of a OidaDB file or *volume*, as we call it, is broken up is
into *pages* of exactly 8192 bytes in length.

A volume's pages are then spit up into *Groups*. A Group is classified
as 1024 pages. With the first 1024 pages belonging to Group 0, the
next 1024 pages belonging to Group 1, and so forth. Hense, a full
group is exactly 8MiB.

Inside each group, the first page is known the *Group Descriptor*, the
remaining 1023 pages are known as the content of that group. The Group
Descriptor details the contents of the group, known as the [[Group Types][Group
Type]].

When you combine a version with a page from a data group, you get what
we call a *[[Blocks]]*. In otherwords, a given block has some data as well
as a number describing the version of said data, everytime the data is
changed, its version is changed.

** Purity
ODB files are by definition pure outputs, that is if the same
operations are performed on 2 newly created ODB files, both of those
files will be exactly the same. This principle means that no where in
a raw ODB file (not including user data) do we store time stamps,
UUIDs, seedless randoms, ect.

Purity does not include the unpreditable properties of
multi-processing: if multiple processes perform the same operations on
two seperate files, there's no guarentee withing the ODB specification
that the order to which the processes' operations will be
predictable. But again, if we're talking about just 1 thread and 1
process, performing the same series of operations on 2 newly created
ODB files, then those two files will be identical.

** Not save to use on a Special Block Device :MVP:
As of now, performing operations on ODB files assume that the
underlying filesystem does what's needed to make sure that calls to
=sync(2)= family functions allow for atomic-writes or rollbacks in
cases of crash.

Although, in the future, I would like to implement a simple journaling
system as well as nessacary checksums to make it so that a database
can be mounted on a special block device (be its own files system).
* Groups
** Group Flags
Group Flags is a 16 bit field with the following description of each
bit.

 - bool 0x01 - denoting the group has been initialized and is in use.
 - mask 0x0C - group type denoting either (0x0) index group, (0x4) data group 
* Blocks
Blocks are the fundemental element to the entire database. Those
wanting to use ODB files will be storing and getting their data in and
from blocks. What /exactly/ is inside of a given block is completely
arbitrary. That is for the user to figure out, the database really
doesn't care what you use the blocks for, just that you can read and
write from and to them quickly and atomically.

A block is defined as a single page of data that has a single version
describing it. If that page of data changes, its version is updated,
but the block ID never changes.

** Block ID
Each block in a given database has its own unique block id. The ID is
synonmous with block offset, meaning block ID 0 is the first block of
the database, block 1 is the second, and so on.

Rather the user of the database be proactive or reactive to using
Block IDs is not defined here. Meaning that if you wish to utilize
"hard-coded" block IDs, that is up to you. Portability dictates you
should probably have a reactive-approach, the same way when you
allocate memory in an operating system you don't exactly know what the
resulting address will be. Likewise, in some applications, using
hard-coded block IDs (or hard-coded address in our memory-allocation
analogy) may have some use to you.

** Atomics
Lets say we want to read the contents of block ID 0. So we go to the
index page and find that block ID 0 is version $V_1$ and its data page
is located at data page $P_1$. We follow $P_1$ and read it's data, but
don't write anything. Now if we go back to that index page, we may
notice that it's version has sense been updated to $V_2$ and its data
page is now $P_2$ has completely been changed... what happened? What
happened to that $P_1$ page we just read?

Well, the way ODB files are designed is that everytime you update a
block, you must create a completely new data page to hold the new
updated data. And only /after/ you're sure that the new data page is
fully written do you update the block's meta data (data page and
version). This is so, if the system crashses before the meta data is
written, then the changes you attempted to write are completely rolled
back. This has also the added benifit of other process having the
older data page mapped (see =mmap(2)=) do not have their copy of data
changed. So going back to our scenario, we now see that our $P_1$ data
page that contains our $V_1$ may actually still exist somewhere in the
database, its just that the block is now pointing to a more up-to-date
copy of the data.

* Block Group

A Block Group is what give us instrunctions as to where to find [[Blocks]]
and their associated data and versions. A block group's descriptor
page has a list of all blocks and all versions, the remaining 1023
pages are known as *Data Pages*. Let's start by looking at the group's
descriptor.

** Block Group Descriptor

| Name   | Type               | Description                                     |
|--------+--------------------+-------------------------------------------------|
| magic  | uint8_t[2]         | ODB Magic number, will always be ~{0xA6, 0xF0}~ |
| flags  | uint16_t           | See [[Group Flags]], the type mask will equal 0x4   |
| rsvd0  | uint32_t           |                                                 |
| blocks | struct block[1023] | see [[Block]]                                       |

** Block
| Name          | Type     | Description       |
|---------------+----------+-------------------|
| data_page_off | uint16_t | [[Data Page Address]] |
| rsvd0         | uint16_t |                   |
| block_ver     | uint32_t | [[Block Version]]     |

We store our ~data_page_off~ as a mere =uint16_t= because we are
listing the offset from the start of the data group rather than
listing the =data_page= offset from the start of the volume.

** TODO Journal

(the following is just an idea, nothing to implement)

Every group has a journal which keeps track of the current commits
being performed on the blocks within the group. Before a commit
starts, the details of the commit are written into the journal, and
only after the commit is complete is it removed from the journal. If
the commit were to crash half way through, then depending on the
commit's state (we'll talk about commit state's later) the database
can rollback the commit, or in some cases, continue to complete it.

You may notice that there is a finite amount of entries of the
journal. This means there is a maximum amount of concurrent commits
that can exist at any one time. It's assumed you're going to run into
I/O limitations far before you have /that/ many CPUs on a single
machine waiting around for journal space.

TODO - we are going to be update multiple blocks at the same time that
are not sequencial. not enough space to use a journal for that. We
need to do inline with the blocks.

| Name         | Type     | Description                                                           |
|--------------+----------+-----------------------------------------------------------------------|
| commit_state | uint8_t  | See [[Commit State]]                                                      |
| commit_name  | uint8_t  | See below                                                             |
| block_len    | uint16_t | Amount of blocks being committed after and including block_start      |
| block_start  | uint32_t | Starting offset of blocks that are being committed for this operation |


| Name       | Type              | Description                                   |
|------------+-------------------+-----------------------------------------------|
| group_off  | odb_gid           | The data page associated with this index page |

| rsvd0      | uint64_t[95]      |                                               |

* ---- In Motion ----
At this point, we have an understanding of every single byte inside of
a given ODB file. However, we have not dived into enough depth to
understand the purpose of some of these bytes. Sure we have all the
data in there, but whats all of the other stuff for? To answer that
we'll transition to the next section of this specifiction: the
database /in motion/. After this point, we'll be exploring the
specification of the software that manages these volumes rather than
the volume itself. Mind you, this document is pretty much agnostic to
the architecture, operating system, language, framework, system calls,
ect you have available to you. Though, there are some assumptions that
OidaDB is built on in practice.

** Multi-processing
OidaDB volumes are designed to be operated on/by multiple different
softwares at once. The practical reasons as to why are up to you. But
rest assured, following this specification you will know that no 2
processes will have undefined race conditions. As discussed in [[Purity]],
this doesn't nessacarly mean the time-based results will be equal
every time. But so long that you implement your merge proceedures
perfectly, the data itself will retain purity even when multiple
processes of unpredictable CPU time are acting upon it at once.

** Syncrounizing
Well, you cannot have a database with an arbitrary amount of processes
acting upon it without talking about syncrounizing could you? Much
effort is put in to trying remove race conditions from the database
without having too many stop lights between processes. It would be
crippling slow to have just a handful of mutexes and at logical
intersections. The key is to identify spots where thread collision is
certainty, try to reduce the need for such collision in the first
place, and with the assumption that thread collision is rare,
implement a cheap-and-sloppy mechanism. In otherwords: where a stop
light could go, we put a round-a-bout.

*** SH Locking
Shared Locking (SH) locking is a type of lock that you can place
anywhere on the volume starting at byte X and ending at byte Y. This
advisory lock tells other processes that the data must not be changed
until the lock is released. Thus, multiple processes can place shared
locks on the same region on the volume and these locks can co-exists.

*** XL Locking
Exclusive Locking (XL) is like a SH Lock, but cannot co-exist with
other locks. XL locks cannot co-exists in the same region as other SH
locks nor other XL locks. This lock advises other process that this
region is about to be edited and thus its contents are volitile until
the lock is released.

*** Weave-locking
The concept of weave locking is /temporarly/ 2 locks at the same
time. For example, starting with creating lock $A$, then creating lock
$B$ and thus simitanously having both lock $A$ and $B$ and only then
releasing lock $A$ to ending up with just lock $B$.

*** AXL Locking
We will be making use of this idea of approximate exclusive locks
(AXL). These locks are a functional extension of XL Locking. Multiple
processes will attempt to place an XL lock on a given piece of data,
and it is assumed that the process to fist get said lock will be
changing the data very quickly, and unlocking and moving
on. Meanwhile, other processes waiting to aquire the lock have already
read the value

*** TXL Locking
Try-exclusive locking (TXL) is where we try to install an XL lock on
something, but we have a back-up plan in the case that such lock
*fails*, such as if there's already an XL/SH lock there.

* Block Reading
** 1) sh-lock block indexes
Place sh locks on the references you are wishing to read. You can
place multiple sh locks at this time so long you do it in order of the
block ids with the lowest being first and the largest being last.

This will prevent any process from trying to update these blocks in
the middle of you reading them.

** 2) for each index: copy the version and block, then unlock
For each block that we've locked, we scan through in the same order to
which we locked them and take down the block's version as well as
copying the datapage to a private buffer.

In step 1 we locked all the blocks at the same time. But in this step
we are actually going to remove the index locks 1-by-1. This allows
for the case that if someone is trying to update, these same blocks,
they don't have to wait until we're completely done, they can follow
right behind us in updating said blocks sense we already copied the
versions we needed.

** TODO Block Corruption
If a block version is ever corrupted, then this means the a block
update had been interupted and never completed. Technically, you can
correctly assume =version - 1= is the correct version of the
referenced data page. But, there is a chance that a page in the Trash
List had been taken out to replace

* Committing
This chapter will cover how to update the contents block in motion.

** 1) xl-lock current block index(es)
Place an xl lock on the block indexes you are about to update. Note,
if you are planning to update multiple blocks, then you can place
multiple locks at this time so long that the lock you place start with
the lowest block id and work your way up to the highest. You can lock
multiple blocks from multiple index pages/index groups.

*defer*: Past this point, regardless of success or failure, unlock all
of these blocks in the same order to which they were locked.

** 2) version check
When updating a series of blocks, you must make sure you applying an
update to the version of blocks to which you expect to apply to. For
example, in [[Block Reading]] we not only copied the block content, we
also copied the block version. So if we had read the block content,
and updated said content, we must make sure that when we save our
changes via this update process we don't accidently clobber someone
else's changes.

*** 2.1) Merge
If any of the blocks we locked do not match the version to which we
expect to see, then unlock all the blocks (starting from lowest ID to
the highest ID), and observe the new version of the blocks and resolve
your changes with the newest versions.

This means, in some cases, steps 1, 2, and 2.1 may repeat in cycle a
few times before the blocks are successfully committed to the database
if these blocks are commonly updated.

** 3) mmap all blocks from disk to memory
With all the block indexes xl locked, you can mmap all of them to
memory (or use a cache here). This means you have 2 bands of memory:
one for the changes you wish to apply, and another for the current
versions. Thus, in order to perform any commit, it is requires you
have 2x memory of the size of the commit.

Furthermore, make sure you have the relevant descriptor pages mmaped
so that you can update the versions as well.

** 4) increment versions and memory transfer

With xl locks placed on all the referneces, all the destination data
pages mmaped, all the destination descriptors mmaped, and you have the
source versions and updates in memory, apply the changes by copying
the source to the destinations. Make sure as you go through, you
increment the versions of the blocks.

Do not make any calls to ~munmap~, ~msync~, ect. until the copying is
fully complete. This will utilize the filesystem and OS to keep
everything atomic when moving from memory to disk.

Only when everything has been copied and versions have been updated
and any calls to syncing the disk, should you then unlock all
references as defined in the defer statement in step 1.
